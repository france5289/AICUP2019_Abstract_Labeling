{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tokenizer import NLTKTokenizer\n",
    "from tokenizer import RegTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "TRAIN_DATA_PATH = os.path.join(CWD, 'data', 'trainset.csv')\n",
    "VALID_DATA_PATH = os.path.join(CWD, 'data', 'validset.csv')\n",
    "TEST_DATA_PATH = os.path.join(CWD, 'data', 'testset.csv')\n",
    "# DICT_PATH = os.path.join(CWD, 'data', 'dictionary.pkl')\n",
    "DICT_PATH = os.path.join(CWD, 'data', 'dictionary_reg.pkl')\n",
    "PAD_TOKEN = '[PAD]'\n",
    "PAD_TOKEN_ID = 0\n",
    "EOS_TOKEN = '[EOS]'\n",
    "EOS_TOKEN_ID = 3\n",
    "\n",
    "# Tokenizer = NLTKTokenizer(  pad_token=PAD_TOKEN,\n",
    "#                             pad_token_id=PAD_TOKEN_ID,\n",
    "#                             eos_token=EOS_TOKEN,\n",
    "#                             eos_token_id=EOS_TOKEN_ID  )\n",
    "\n",
    "\n",
    "Tokenizer_Reg = RegTokenizer(   pad_token=PAD_TOKEN,\n",
    "                                pad_token_id=PAD_TOKEN_ID,\n",
    "                                eos_token=EOS_TOKEN,\n",
    "                                eos_token_id=EOS_TOKEN_ID  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDict(train, valid):\n",
    "    # global Tokenizer\n",
    "    global Tokenizer_Reg\n",
    "    if os.path.exists(DICT_PATH):\n",
    "        # Tokenizer = NLTKTokenizer.load_from_file(DICT_PATH)\n",
    "        Tokenizer_Reg = RegTokenizer.load_from_file(DICT_PATH)\n",
    "    else:\n",
    "        for item in tqdm(train['Abstract'], desc='Train set'):\n",
    "            # Tokenizer.build_dict([item])\n",
    "            Tokenizer_Reg.build_dict([item], min_count=5)\n",
    "        for item in tqdm(valid['Abstract'], desc='Valid set'):\n",
    "            # Tokenizer.build_dict([item])\n",
    "            Tokenizer_Reg.build_dict([item], min_count=5)\n",
    "        # Tokenizer.save_to_file(DICT_PATH)\n",
    "        Tokenizer_Reg.save_to_file(DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Generate relative dictionary\n"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH, dtype=str)\n",
    "valid = pd.read_csv(VALID_DATA_PATH, dtype=str)\n",
    "test = pd.read_csv(TEST_DATA_PATH, dtype=str)\n",
    "print('Generate relative dictionary')\n",
    "GenDict(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OOV(word_dict, wordvector_path, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(wordvector_path)\n",
    "    for line in f:\n",
    "        values = line.replace(',','').split()\n",
    "        token = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[token] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # max_words = Tokenizer.vocab_size()\n",
    "    # embedding_matrix = np.random.randn(max_words, embedding_dim)\n",
    "\n",
    "    OOV_list = []\n",
    "    for token, index in word_dict.items():\n",
    "        embedding_vector = embeddings_index.get(token)\n",
    "        # if embedding_vector is not None:\n",
    "            # embedding_matrix[index] = embedding_vector\n",
    "        if embedding_vector is None:\n",
    "            OOV_list.append(token)\n",
    "\n",
    "    return OOV_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "131166"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>131161</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>131162</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>131163</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>131164</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>131165</th>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>131166 rows Ã— 1 columns</p>\n</div>",
      "text/plain": "        Abstract\n0          False\n1          False\n2          False\n3          False\n4          False\n...          ...\n131161     False\n131162     False\n131163     False\n131164     False\n131165     False\n\n[131166 rows x 1 columns]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Abstract    True\nName: 26369, dtype: bool"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().iloc[26369,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "26369     NaN\n58459     NaN\n61649     NaN\n122529    NaN\n126428    NaN\nName: Abstract, dtype: object"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Abstract'][test.isna()['Abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 1425892 word vectors.\n"
    }
   ],
   "source": [
    "# oov_list = get_OOV(Tokenizer.get_token_to_id(), f'glove/glove_512d.txt', 512)\n",
    "oov_list = get_OOV(Tokenizer_Reg.get_token_to_id(), f'glove/glove_512d.txt', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oov_words_RegTokenize.txt', 'w') as f:\n",
    "    for word in oov_list:\n",
    "        f.write(word)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}