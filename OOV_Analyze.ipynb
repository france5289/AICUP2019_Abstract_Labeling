{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tokenizer import NLTKTokenizer\n",
    "from tokenizer import RegTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "TRAIN_DATA_PATH = os.path.join(CWD, 'data', 'trainset.csv')\n",
    "VALID_DATA_PATH = os.path.join(CWD, 'data', 'validset.csv')\n",
    "TEST_DATA_PATH = os.path.join(CWD, 'data', 'testset.csv')\n",
    "# DICT_PATH = os.path.join(CWD, 'data', 'dictionary.pkl')\n",
    "DICT_PATH = os.path.join(CWD, 'data', 'dictionary_reg.pkl')\n",
    "PAD_TOKEN = '[PAD]'\n",
    "PAD_TOKEN_ID = 0\n",
    "EOS_TOKEN = '[EOS]'\n",
    "EOS_TOKEN_ID = 3\n",
    "\n",
    "# Tokenizer = NLTKTokenizer(  pad_token=PAD_TOKEN,\n",
    "#                             pad_token_id=PAD_TOKEN_ID,\n",
    "#                             eos_token=EOS_TOKEN,\n",
    "#                             eos_token_id=EOS_TOKEN_ID  )\n",
    "\n",
    "\n",
    "Tokenizer_Reg = RegTokenizer(   pad_token=PAD_TOKEN,\n",
    "                                pad_token_id=PAD_TOKEN_ID,\n",
    "                                eos_token=EOS_TOKEN,\n",
    "                                eos_token_id=EOS_TOKEN_ID  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDict(train, valid):\n",
    "    # global Tokenizer\n",
    "    global Tokenizer_Reg\n",
    "    if os.path.exists(DICT_PATH):\n",
    "        # Tokenizer = NLTKTokenizer.load_from_file(DICT_PATH)\n",
    "        Tokenizer_Reg = RegTokenizer.load_from_file(DICT_PATH)\n",
    "    else:\n",
    "        for item in tqdm(train['Abstract'], desc='Train set'):\n",
    "            # Tokenizer.build_dict([item])\n",
    "            Tokenizer_Reg.build_dict([item])\n",
    "        for item in tqdm(valid['Abstract'], desc='Valid set'):\n",
    "            # Tokenizer.build_dict([item])\n",
    "            Tokenizer_Reg.build_dict([item])\n",
    "        # Tokenizer.save_to_file(DICT_PATH)\n",
    "        Tokenizer_Reg.save_to_file(DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train set:  34%|███▍      | 14457/42180 [00:00<00:00, 144567.38it/s]Generate relative dictionary\nTrain set: 100%|██████████| 42180/42180 [00:00<00:00, 147212.03it/s]\nValid set: 100%|██████████| 4687/4687 [00:00<00:00, 149039.09it/s]\n"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "valid = pd.read_csv(VALID_DATA_PATH)\n",
    "test = pd.read_csv(TEST_DATA_PATH)\n",
    "print('Generate relative dictionary')\n",
    "GenDict(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OOV(word_dict, wordvector_path, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(wordvector_path)\n",
    "    for line in f:\n",
    "        values = line.replace(',','').split()\n",
    "        token = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[token] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # max_words = Tokenizer.vocab_size()\n",
    "    # embedding_matrix = np.random.randn(max_words, embedding_dim)\n",
    "\n",
    "    OOV_list = []\n",
    "    for token, index in word_dict.items():\n",
    "        embedding_vector = embeddings_index.get(token)\n",
    "        # if embedding_vector is not None:\n",
    "            # embedding_matrix[index] = embedding_vector\n",
    "        if embedding_vector is None:\n",
    "            OOV_list.append(token)\n",
    "\n",
    "    return OOV_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 1425892 word vectors.\n"
    }
   ],
   "source": [
    "# oov_list = get_OOV(Tokenizer.get_token_to_id(), f'glove/glove_512d.txt', 512)\n",
    "oov_list = get_OOV(Tokenizer_Reg.get_token_to_id(), f'glove/glove_512d.txt', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2089"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oov_words_RegTokenize.txt', 'w') as f:\n",
    "    for word in oov_list:\n",
    "        f.write(word)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}