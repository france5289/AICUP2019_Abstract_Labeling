{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tokenizer import NLTKTokenizer\n",
    "\n",
    "CWD = os.getcwd()\n",
    "TRAIN_DATA_PATH = os.path.join(CWD, 'data', 'trainset.csv')\n",
    "VALID_DATA_PATH = os.path.join(CWD, 'data', 'validset.csv')\n",
    "TEST_DATA_PATH = os.path.join(CWD, 'data', 'testset.csv')\n",
    "DICT_PATH = os.path.join(CWD, 'data', 'dictionary.pkl')\n",
    "WORKERS = os.cpu_count() // 2\n",
    "Tokenizer = NLTKTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitSent(doc):\n",
    "    return doc.split('$$$')\n",
    "\n",
    "\n",
    "def GenDict(train, valid):\n",
    "    global Tokenizer\n",
    "    if os.path.exists(DICT_PATH):\n",
    "        Tokenizer = NLTKTokenizer.load_from_file(DICT_PATH)\n",
    "    else:\n",
    "        for item in train['Abstract']:\n",
    "            Tokenizer.build_dict(item)\n",
    "\n",
    "        for item in valid['Abstract']:\n",
    "            Tokenizer.build_dict(item)\n",
    "\n",
    "        Tokenizer.save_to_file(DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "valid = pd.read_csv(VALID_DATA_PATH)\n",
    "test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "train['Abstract'] = train['Abstract'].apply(func=SplitSent)\n",
    "valid['Abstract'] = valid['Abstract'].apply(func=SplitSent)\n",
    "GenDict(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Abstract'] = test['Abstract'].apply(func=SplitSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels):\n",
    "    '''\n",
    "    Convert labels to one-hot encoding\n",
    "\n",
    "    Args : \n",
    "        labels:( DataFrame column item ) \n",
    "    Return :\n",
    "        one_hot_labels: ( DataFrame column item )\n",
    "    '''\n",
    "    one_hot_labels = []\n",
    "    label_list = labels.split(' ')\n",
    "    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "    for label in label_list:\n",
    "        onehot = [0,0,0,0,0,0]\n",
    "        for l in label.split('/'):\n",
    "            onehot[label_dict[l]] = 1\n",
    "        one_hot_labels.append(onehot)\n",
    "    \n",
    "    return one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(dataset):\n",
    "    '''\n",
    "    encode 'Abstract' and convert label to one_hot\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset(pd.DataFrame)\n",
    "    '''\n",
    "    global Tokenizer\n",
    "    dataset['Abstract'] = dataset['Abstract'].apply(func=Tokenizer.encode)\n",
    "    if 'Task 1' in dataset.columns:\n",
    "        dataset['Task 1'] = dataset['Task 1'].apply(func=labels_to_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data(train)\n",
    "encode_data(valid)\n",
    "encode_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Abstract    [[42485, 35732, 42367, 42450, 38674, 10, 42508...\nTask 1      [[1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 1...\nName: 0, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract(Dataset):\n",
    "    def __init__(self, data, pad_idx, eos_token):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_token = eos_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.index)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index]\n",
    "    \n",
    "    def collate_fn(self, datas):\n",
    "        abstracts = [ torch.as_tensor( abstract, dtype = torch.long ) for data in datas for abstract in data['Abstract'] ]\n",
    "        batch_abstracts = pad_sequence( abstracts, batch_first = True, padding_value = self.pad_idx )\n",
    "\n",
    "        labels = [ torch.as_tensor( label, dtype = torch.float) for data in datas for label in data['Task 1'] ]\n",
    "        batch_labels = pad_sequence( labels, batch_first = True, padding_value = self.pad_idx )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Abstract(data=train, pad_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Abstract    [[42485, 35732, 42367, 42450, 38674, 10, 42508...\nTask 1      [[1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 1...\nName: 0, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ torch.as_tensor(abstract, dtype=torch.float) for abstract in dataset[0]['Task 1'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([1., 0., 0., 0., 0., 0.]),\n tensor([1., 0., 0., 0., 0., 0.]),\n tensor([1., 1., 0., 0., 0., 0.]),\n tensor([0., 0., 1., 0., 0., 0.]),\n tensor([0., 0., 1., 0., 0., 0.]),\n tensor([0., 0., 1., 1., 0., 0.])]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 1., 1., 0., 0.]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels = pad_sequence( labels, batch_first=True )\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [ torch.as_tensor(vec) for  vec in dataset[0]['Abstract'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([42485, 35732, 42367, 42450, 38674,    10, 42508, 42313, 42472, 38387,\n         42474, 42459, 42455, 42471, 42228, 42500, 42356, 42451, 42463, 42441,\n         42463, 42416, 42306, 42468, 42437, 42396, 42445, 42526, 42450, 42105,\n         42115, 42451, 42550, 42366, 42465,     3]),\n tensor([42007, 42500, 42497, 42204, 42502, 42035, 42438, 42453, 42451, 42497,\n         42514, 42502, 42531, 41078, 42556, 42515, 40045, 42459, 42452, 42497,\n         40188, 41599, 42465,     3]),\n tensor([41532, 42488, 42453, 42351, 42451, 42455, 42404, 42502, 42541, 42555,\n         42445, 42515, 40045, 42507, 42432, 42495, 42500, 42435, 42416, 42463,\n         42516, 42468, 42531, 42541, 42499, 41271, 42497, 42484, 42481, 42455,\n         42360, 42476, 42465,     3]),\n tensor([42516, 42453, 42518, 42451, 42519, 42396, 42455, 42517, 42481, 42424,\n         42515, 40045, 42451, 42466,    80, 42451, 42469, 42459, 42407, 42445,\n         42396, 42452, 42455, 41566, 42299, 42502, 42436, 42451, 42474, 42424,\n         42515, 40045, 42499, 42454, 42465, 42488, 42451, 42472, 42445, 42469,\n         42242, 42451, 42463, 42077, 42445, 42393, 41690, 42489, 42386, 42188,\n         42465,     3]),\n tensor([42465, 42501, 42502, 42455, 42404, 42502, 38680, 42475, 42451, 42500,\n         42535, 42446, 42463, 42263, 42451, 42485, 42452, 42477, 42366, 42502,\n         42102, 42505, 42475, 42451, 42323, 42442, 42475, 42463, 42494, 42453,\n         42492, 41231, 42465,     3]),\n tensor([35132, 42468, 42497,    80, 42519, 42462, 42455, 42349, 42463, 42455,\n         42536, 42442, 42481, 42455, 40505, 42306, 42502, 41949, 42499, 42389,\n         42492, 42505, 42514, 42480, 42451, 42485, 42452, 42497, 42117, 42237,\n         42451, 42445, 42380, 42394, 42555, 42451, 42485, 42452, 42497,   133,\n         42448, 42486, 42451, 42463, 42444, 42496, 42455, 42476, 42468, 42244,\n         40736, 42465,     3])]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([36])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[42485, 35732, 42367, 42450, 38674,    10, 42508, 42313, 42472, 38387,\n         42474, 42459, 42455, 42471, 42228, 42500, 42356, 42451, 42463, 42441,\n         42463, 42416, 42306, 42468, 42437, 42396, 42445, 42526, 42450, 42105,\n         42115, 42451, 42550, 42366, 42465,     3,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [42007, 42500, 42497, 42204, 42502, 42035, 42438, 42453, 42451, 42497,\n         42514, 42502, 42531, 41078, 42556, 42515, 40045, 42459, 42452, 42497,\n         40188, 41599, 42465,     3,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [41532, 42488, 42453, 42351, 42451, 42455, 42404, 42502, 42541, 42555,\n         42445, 42515, 40045, 42507, 42432, 42495, 42500, 42435, 42416, 42463,\n         42516, 42468, 42531, 42541, 42499, 41271, 42497, 42484, 42481, 42455,\n         42360, 42476, 42465,     3,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [42516, 42453, 42518, 42451, 42519, 42396, 42455, 42517, 42481, 42424,\n         42515, 40045, 42451, 42466,    80, 42451, 42469, 42459, 42407, 42445,\n         42396, 42452, 42455, 41566, 42299, 42502, 42436, 42451, 42474, 42424,\n         42515, 40045, 42499, 42454, 42465, 42488, 42451, 42472, 42445, 42469,\n         42242, 42451, 42463, 42077, 42445, 42393, 41690, 42489, 42386, 42188,\n         42465,     3,     0],\n        [42465, 42501, 42502, 42455, 42404, 42502, 38680, 42475, 42451, 42500,\n         42535, 42446, 42463, 42263, 42451, 42485, 42452, 42477, 42366, 42502,\n         42102, 42505, 42475, 42451, 42323, 42442, 42475, 42463, 42494, 42453,\n         42492, 41231, 42465,     3,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [35132, 42468, 42497,    80, 42519, 42462, 42455, 42349, 42463, 42455,\n         42536, 42442, 42481, 42455, 40505, 42306, 42502, 41949, 42499, 42389,\n         42492, 42505, 42514, 42480, 42451, 42485, 42452, 42497, 42117, 42237,\n         42451, 42445, 42380, 42394, 42555, 42451, 42485, 42452, 42497,   133,\n         42448, 42486, 42451, 42463, 42444, 42496, 42455, 42476, 42468, 42244,\n         40736, 42465,     3]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = pad_sequence(sequence, batch_first=True)\n",
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6, 53])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset = dataset, batch_size = 2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}