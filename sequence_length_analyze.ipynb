{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle\n",
    "from tokenizer import NLTKTokenizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "TRAIN_DATA_PATH = os.path.join(CWD, 'data', 'trainset.csv')\n",
    "VALID_DATA_PATH = os.path.join(CWD, 'data', 'validset.csv')\n",
    "TEST_DATA_PATH = os.path.join(CWD, 'data', 'testset.csv')\n",
    "DICT_PATH = os.path.join(CWD, 'data', 'dictionary.pkl')\n",
    "WORKERS = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = NLTKTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "valid = pd.read_csv(VALID_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 6300/6300 [00:00<00:00, 371480.00it/s]\n100%|██████████| 700/700 [00:00<00:00, 252973.70it/s]\n"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train['Abstract'] = train['Abstract'].progress_apply(func=lambda doc : doc.split('$$$'))\n",
    "valid['Abstract'] = valid['Abstract'].progress_apply(func=lambda doc : doc.split('$$$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDict(train, valid):\n",
    "    global Tokenizer\n",
    "    if os.path.exists(DICT_PATH):\n",
    "        Tokenizer = NLTKTokenizer.load_from_file(DICT_PATH)\n",
    "    else:\n",
    "        for item in train['Abstract']:\n",
    "            Tokenizer.build_dict(item)\n",
    "\n",
    "        for item in valid['Abstract']:\n",
    "            Tokenizer.build_dict(item)\n",
    "        Tokenizer.save_to_file(DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenDict(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def labels_to_onehot(labels):\n",
    "    '''\n",
    "    Convert labels to one-hot encoding\n",
    "\n",
    "    Args : \n",
    "        labels:( DataFrame column item ) \n",
    "    Return :\n",
    "        one_hot_labels: ( DataFrame column item )\n",
    "    '''\n",
    "    one_hot_labels = []\n",
    "    label_list = labels.split(' ')\n",
    "    label_dict = {\n",
    "        'BACKGROUND': 0,\n",
    "        'OBJECTIVES': 1,\n",
    "        'METHODS': 2,\n",
    "        'RESULTS': 3,\n",
    "        'CONCLUSIONS': 4,\n",
    "        'OTHERS': 5\n",
    "    }\n",
    "    for label in label_list:\n",
    "        onehot = [0, 0, 0, 0, 0, 0]\n",
    "        for l in label.split('/'):\n",
    "            onehot[label_dict[l]] = 1\n",
    "        one_hot_labels.append(onehot)\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def encode_data(dataset):\n",
    "    '''\n",
    "    encode 'Abstract' and convert label to one_hot\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataset(pd.DataFrame)\n",
    "    '''\n",
    "    global Tokenizer\n",
    "    tqdm.pandas()\n",
    "    dataset['Abstract'] = dataset['Abstract'].progress_apply(func=Tokenizer.encode)\n",
    "    if 'Task 1' in dataset.columns:\n",
    "        dataset['Task 1'] = dataset['Task 1'].progress_apply(func=labels_to_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 6300/6300 [00:04<00:00, 1267.70it/s]\n100%|██████████| 6300/6300 [00:00<00:00, 216481.09it/s]\n100%|██████████| 700/700 [00:00<00:00, 1233.99it/s]\n100%|██████████| 700/700 [00:00<00:00, 91761.87it/s]\n"
    }
   ],
   "source": [
    "encode_data(train)\n",
    "encode_data(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMaxSeqLength(abstract):\n",
    "    max = 0\n",
    "    for sent in abstract:\n",
    "        if len(sent) > max:\n",
    "            max = len(sent)\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 6300/6300 [00:00<00:00, 548878.63it/s]\n"
    }
   ],
   "source": [
    "train['MaxSeqLength'] = train['Abstract'].progress_apply(func=GetMaxSeqLength) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "count    6300.000000\nmean       27.181270\nstd         8.704375\nmin         9.000000\n25%        22.000000\n50%        26.000000\n75%        31.000000\nmax       125.000000\nName: MaxSeqLength, dtype: float64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['MaxSeqLength'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 700/700 [00:00<00:00, 307146.44it/s]\n"
    }
   ],
   "source": [
    "valid['MaxSeqLength'] = valid['Abstract'].progress_apply(func=GetMaxSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "count    700.000000\nmean      27.008571\nstd        8.079359\nmin        8.000000\n25%       22.000000\n50%       25.000000\n75%       31.000000\nmax       77.000000\nName: MaxSeqLength, dtype: float64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['MaxSeqLength'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}