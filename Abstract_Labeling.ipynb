{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle, json, re, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.parsing import remove_stopwords\n",
    "\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "DATA_PATH = os.path.join(CWD, 'data')\n",
    "CPUNUM = os.cpu_count() // 2\n",
    "print(CPUNUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-3803b2ac7dae>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3803b2ac7dae>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    For tuning best models, you may need to save the used hyperparameters.<br />\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter logging and tuning\n",
    "For tuning best models, you may need to save the used hyperparameters.<br />\n",
    "The two cells below make the logging convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def write_config(filename, with_time=False):\n",
    "    config = configparser.ConfigParser()\n",
    "    config['DEFAULT'] = {'embedding_dim': embedding_dim,\n",
    "                         'hidden_dim': hidden_dim,\n",
    "                         'learning_rate': learning_rate,\n",
    "                         'max_epoch': max_epoch,\n",
    "                         'batch_size': batch_size}\n",
    "    \n",
    "    if with_time == False:\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return 'config'            \n",
    "    else:\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = filename + '_' + timestr\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return ( 'config' + timestr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters tuning\n",
    "### Run this cell for renewing the hyperparameters\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-4\n",
    "max_epoch = 10\n",
    "batch_size = 16\n",
    "\n",
    "config_fname = write_config(os.path.join(CWD,\"config\"), True)\n",
    "# config_fname will be used when logging training scalar to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv( os.path.join(DATA_PATH,'task1_trainset.csv'), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Authors</th>\n      <th>Categories</th>\n      <th>Created Date</th>\n      <th>Task 1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>D00001</td>\n      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n      <td>cs.CR/cs.AI/q-bio.NC</td>\n      <td>2018-01-11</td>\n      <td>BACKGROUND OBJECTIVES METHODS METHODS RESULTS ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D00002</td>\n      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n      <td>In this paper, we address the problem of compu...</td>\n      <td>Sadeghi/Smith</td>\n      <td>cs.SY/cs.RO/math.OC</td>\n      <td>2016-09-21</td>\n      <td>OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D00003</td>\n      <td>Data-driven Upsampling of Point Clouds</td>\n      <td>High quality upsampling of sparse 3D point clo...</td>\n      <td>Zhang/Jiang/Yang/Yamakawa/Shimada/Kara</td>\n      <td>cs.CV</td>\n      <td>2018-07-07</td>\n      <td>BACKGROUND OBJECTIVES METHODS METHODS METHODS ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D00004</td>\n      <td>Accessibility or Usability of InteractSE? A He...</td>\n      <td>Internet is the main source of information now...</td>\n      <td>Aqle/Khowaja/Al-Thani</td>\n      <td>cs.HC</td>\n      <td>2018-08-29</td>\n      <td>BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OB...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D00005</td>\n      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n      <td>Automated Facial Expression Recognition (FER) ...</td>\n      <td>Hasani/Mahoor</td>\n      <td>cs.CV</td>\n      <td>2017-03-20</td>\n      <td>BACKGROUND BACKGROUND BACKGROUND BACKGROUND ME...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       Id                                              Title  \\\n0  D00001  A Brain-Inspired Trust Management Model to Ass...   \n1  D00002  On Efficient Computation of Shortest Dubins Pa...   \n2  D00003             Data-driven Upsampling of Point Clouds   \n3  D00004  Accessibility or Usability of InteractSE? A He...   \n4  D00005  Spatio-Temporal Facial Expression Recognition ...   \n\n                                            Abstract  \\\n0  Rapid popularity of Internet of Things (IoT) a...   \n1  In this paper, we address the problem of compu...   \n2  High quality upsampling of sparse 3D point clo...   \n3  Internet is the main source of information now...   \n4  Automated Facial Expression Recognition (FER) ...   \n\n                                             Authors            Categories  \\\n0  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n1                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n2             Zhang/Jiang/Yang/Yamakawa/Shimada/Kara                 cs.CV   \n3                              Aqle/Khowaja/Al-Thani                 cs.HC   \n4                                      Hasani/Mahoor                 cs.CV   \n\n  Created Date                                             Task 1  \n0   2018-01-11  BACKGROUND OBJECTIVES METHODS METHODS RESULTS ...  \n1   2016-09-21  OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS  \n2   2018-07-07  BACKGROUND OBJECTIVES METHODS METHODS METHODS ...  \n3   2018-08-29  BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OB...  \n4   2017-03-20  BACKGROUND BACKGROUND BACKGROUND BACKGROUND ME...  "
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove (current) redundant columns.\n",
    "\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "#dataset['Task 1'] = dataset['Task 1'].str.lower()\n",
    "\n",
    "for i in range(len(dataset['Abstract'])):\n",
    "    dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Abstract</th>\n      <th>Task 1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>D00001</td>\n      <td>rapid popularity internet things (iot) cloud c...</td>\n      <td>BACKGROUND OBJECTIVES METHODS METHODS RESULTS ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D00002</td>\n      <td>paper, address problem computing optimal paths...</td>\n      <td>OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D00003</td>\n      <td>high quality upsampling sparse 3d point clouds...</td>\n      <td>BACKGROUND OBJECTIVES METHODS METHODS METHODS ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D00004</td>\n      <td>internet main source information nowadays.$$$t...</td>\n      <td>BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OB...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D00005</td>\n      <td>automated facial expression recognition (fer) ...</td>\n      <td>BACKGROUND BACKGROUND BACKGROUND BACKGROUND ME...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "       Id                                           Abstract  \\\n0  D00001  rapid popularity internet things (iot) cloud c...   \n1  D00002  paper, address problem computing optimal paths...   \n2  D00003  high quality upsampling sparse 3d point clouds...   \n3  D00004  internet main source information nowadays.$$$t...   \n4  D00005  automated facial expression recognition (fer) ...   \n\n                                              Task 1  \n0  BACKGROUND OBJECTIVES METHODS METHODS RESULTS ...  \n1  OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS  \n2  BACKGROUND OBJECTIVES METHODS METHODS METHODS ...  \n3  BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OB...  \n4  BACKGROUND BACKGROUND BACKGROUND BACKGROUND ME...  "
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test_size=0.1 for validation split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv(os.path.join(DATA_PATH,'trainset.csv'),index=False)\n",
    "validset.to_csv(os.path.join(DATA_PATH,'validset.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove (current) redundant columns of the test set.\n",
    "dataset = pd.read_csv(os.path.join(DATA_PATH, 'task1_public_testset.csv'), dtype=str)\n",
    "dataset.drop('Title', axis=1, inplace=True)\n",
    "dataset.drop('Categories', axis=1, inplace=True)\n",
    "dataset.drop('Created Date', axis=1, inplace=True)\n",
    "dataset.drop('Authors', axis=1, inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "\n",
    "dataset['Abstract'] = dataset['Abstract'].apply(func=remove_stopwords)\n",
    "\n",
    "dataset.to_csv(os.path.join(DATA_PATH, 'testset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collect words and create the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def collect_words(data_path, n_workers=8):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "    # create a list for storing sentences\n",
    "    sent_list = []\n",
    "    for _ , row in df.iterrows():\n",
    "        # remove $$$ and append to sent_list\n",
    "        sent_list.extend(row['Abstract'].split('$$$'))\n",
    "\n",
    "    chunks = [\n",
    "        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n",
    "        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n",
    "    ]\n",
    "    with Pool(n_workers) as pool:\n",
    "        # word_tokenize for word-word separation\n",
    "        chunks = pool.map_async(word_tokenize, chunks)\n",
    "        words = set(sum(chunks.get(), []))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words(os.path.join(DATA_PATH, 'trainset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download Glove pretrained word embedding from web.\n",
    "\n",
    "Link: http://nlp.stanford.edu/data/glove.6B.zip <br />\n",
    "It takes about 5 minutes for the download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "if not os.path.exists('glove'):\n",
    "    os.mkdir('glove')\n",
    "    r = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path='glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing the GloVe word-embeddings file\n",
    "\n",
    "Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 400000 word vectors\n"
    }
   ],
   "source": [
    "# Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)\n",
    "wordvector_path = 'glove/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "with open(wordvector_path) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing the GloVe word-embeddings matrix\n",
    "\n",
    "max_words = len(word_dict)\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_dict.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Formatting\n",
    "建立完字典後，我們要將 data 切成數個 batch，並且將輸入的句子轉成數字，將答案轉成 onehot vector。\n",
    "- `label_to_onehot(labels)`:  \n",
    "    將 datasert 中的 label string 轉成 onehot encoding vector。  \n",
    "- `sentence_to_indices(sentence, word_dict)`:  \n",
    "    將輸入的句子中每個 word 轉成字典中對應的 index  \n",
    "    ex : 'i love ncku' -> $[1,2,3]$\n",
    "- `get_dataset(data_path, word_dict, n_workers=4)`:  \n",
    "    將 dataset 讀入\n",
    "- `preprocess_samples(dataset, word_dict)`:  \n",
    "    傳入所有 input sentences 並進行 data preprocessing  \n",
    "- `preprocess_sample(data, word_dict)`:  \n",
    "    主要透過這個 function 移除存在於 'Abstract' 中的 `$` 符號  \n",
    "    並將 'Label' 轉成 onehot encoding vector。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "    onehot = [0,0,0,0,0,0]\n",
    "    for l in labels.split('/'):\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    ## clean abstracts by removing $$$\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    \n",
    "    ## convert the labels into one-hot encoding\n",
    "    if 'Task 1' in data:\n",
    "        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset(os.path.join(DATA_PATH,'trainset.csv'), word_dict, n_workers=8)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset(os.path.join(DATA_PATH,'validset.csv'), word_dict, n_workers=8)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset(os.path.join(DATA_PATH,'testset.csv'), word_dict, n_workers=8)"
   ]
  }
 ]
}