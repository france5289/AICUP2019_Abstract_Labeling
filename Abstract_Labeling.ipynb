{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle, json, re, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.parsing import remove_stopwords\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "DATA_PATH = os.path.join(CWD, 'data')\n",
    "CPUNUM = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-3803b2ac7dae>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3803b2ac7dae>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    For tuning best models, you may need to save the used hyperparameters.<br />\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter logging and tuning\n",
    "For tuning best models, you may need to save the used hyperparameters.<br />\n",
    "The two cells below make the logging convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def write_config(filename, with_time=False):\n",
    "    config = configparser.ConfigParser()\n",
    "    config['DEFAULT'] = {'embedding_dim': embedding_dim,\n",
    "                         'hidden_dim': hidden_dim,\n",
    "                         'learning_rate': learning_rate,\n",
    "                         'max_epoch': max_epoch,\n",
    "                         'batch_size': batch_size,\n",
    "                         'drop_p' : drop_p\n",
    "                         }\n",
    "    \n",
    "    if with_time == False:\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return 'config'            \n",
    "    else:\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = filename + '_' + timestr\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return ( 'config' + timestr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters tuning\n",
    "### Run this cell for renewing the hyperparameters\n",
    "\n",
    "embedding_dim = 100 # word embedding dim for Glove\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-4\n",
    "max_epoch = 10\n",
    "batch_size = 16\n",
    "drop_p = 0.2\n",
    "\n",
    "config_fname = write_config(os.path.join(CWD,\"config\"), True)\n",
    "# config_fname will be used when logging training scalar to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv( os.path.join(DATA_PATH,'task1_trainset.csv'), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove (current) redundant columns.\n",
    "\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "#dataset['Task 1'] = dataset['Task 1'].str.lower()\n",
    "\n",
    "for i in range(len(dataset['Abstract'])):\n",
    "    dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test_size=0.1 for validation split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv(os.path.join(DATA_PATH,'trainset.csv'),index=False)\n",
    "validset.to_csv(os.path.join(DATA_PATH,'validset.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove (current) redundant columns of the test set.\n",
    "dataset = pd.read_csv(os.path.join(DATA_PATH, 'task1_public_testset.csv'), dtype=str)\n",
    "dataset.drop('Title', axis=1, inplace=True)\n",
    "dataset.drop('Categories', axis=1, inplace=True)\n",
    "dataset.drop('Created Date', axis=1, inplace=True)\n",
    "dataset.drop('Authors', axis=1, inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "\n",
    "dataset['Abstract'] = dataset['Abstract'].apply(func=remove_stopwords)\n",
    "\n",
    "dataset.to_csv(os.path.join(DATA_PATH, 'testset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collect words and create the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def collect_words(data_path, n_workers=4):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "    # create a list for storing sentences\n",
    "    sent_list = []\n",
    "    for _ , row in df.iterrows():\n",
    "        # remove $$$ and append to sent_list\n",
    "        sent_list.extend(row['Abstract'].split('$$$'))\n",
    "\n",
    "    chunks = [\n",
    "        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n",
    "        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n",
    "    ]\n",
    "    with Pool(n_workers) as pool:\n",
    "        # word_tokenize for word-word separation\n",
    "        chunks = pool.map_async(word_tokenize, chunks)\n",
    "        words = set(sum(chunks.get(), []))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words(os.path.join(DATA_PATH, 'trainset.csv'), n_workers=CPUNUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download Glove pretrained word embedding from web.\n",
    "\n",
    "Link: http://nlp.stanford.edu/data/glove.6B.zip <br />\n",
    "It takes about 5 minutes for the download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "if not os.path.exists('glove'):\n",
    "    os.mkdir('glove')\n",
    "    r = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path='glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing the GloVe word-embeddings file\n",
    "\n",
    "Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 400000 word vectors\n"
    }
   ],
   "source": [
    "# Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)\n",
    "wordvector_path = 'glove/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "with open(wordvector_path) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing the GloVe word-embeddings matrix\n",
    "\n",
    "max_words = len(word_dict)\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_dict.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Formatting\n",
    "建立完字典後，我們要將 data 切成數個 batch，並且將輸入的句子轉成數字，將答案轉成 onehot vector。\n",
    "- `label_to_onehot(labels)`:  \n",
    "    將 datasert 中的 label string 轉成 onehot encoding vector。  \n",
    "- `sentence_to_indices(sentence, word_dict)`:  \n",
    "    將輸入的句子中每個 word 轉成字典中對應的 index  \n",
    "    ex : 'i love ncku' -> $[1,2,3]$\n",
    "- `get_dataset(data_path, word_dict, n_workers=4)`:  \n",
    "    將 dataset 讀入\n",
    "- `preprocess_samples(dataset, word_dict)`:  \n",
    "    傳入所有 input sentences 並進行 data preprocessing  \n",
    "- `preprocess_sample(data, word_dict)`:  \n",
    "    主要透過這個 function 移除存在於 'Abstract' 中的 `$` 符號  \n",
    "    並將 'Label' 轉成 onehot encoding vector。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "    onehot = [0,0,0,0,0,0]\n",
    "    for l in labels.split('/'):\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    ## clean abstracts by removing $$$\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    \n",
    "    ## convert the labels into one-hot encoding\n",
    "    if 'Task 1' in data:\n",
    "        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[INFO] Start processing trainset...\n100%|██████████| 787/787 [00:00<00:00, 952.67it/s]\n100%|██████████| 787/787 [00:00<00:00, 906.39it/s]\n100%|██████████| 787/787 [00:00<00:00, 861.71it/s]\n\n100%|██████████| 791/791 [00:00<00:00, 868.09it/s]\n100%|██████████| 787/787 [00:00<00:00, 794.86it/s]\n100%|██████████| 787/787 [00:00<00:00, 842.91it/s]\n100%|██████████| 787/787 [00:00<00:00, 792.73it/s]\n[INFO] Start processing validset...\n 91%|█████████ | 79/87 [00:00<00:00, 779.39it/s]\n100%|██████████| 87/87 [00:00<00:00, 782.57it/s]\n100%|██████████| 87/87 [00:00<00:00, 765.11it/s]\n100%|██████████| 87/87 [00:00<00:00, 844.27it/s]\n 92%|█████████▏| 80/87 [00:00<00:00, 790.73it/s]\n 87%|████████▋ | 76/87 [00:00<00:00, 753.11it/s]\n100%|██████████| 91/91 [00:00<00:00, 955.70it/s]\n100%|██████████| 87/87 [00:00<00:00, 730.61it/s]\n[INFO] Start processing testset...\n 96%|█████████▌| 2388/2500 [00:02<00:00, 895.48it/s]\n100%|██████████| 2500/2500 [00:02<00:00, 870.70it/s]\n\n100%|██████████| 2500/2500 [00:02<00:00, 865.84it/s]\n100%|██████████| 2500/2500 [00:02<00:00, 851.82it/s]\n100%|██████████| 2500/2500 [00:02<00:00, 834.48it/s]\n100%|██████████| 2500/2500 [00:02<00:00, 844.55it/s]\n100%|██████████| 2500/2500 [00:02<00:00, 838.33it/s]\n"
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset(os.path.join(DATA_PATH,'trainset.csv'), word_dict, n_workers=CPUNUM)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset(os.path.join(DATA_PATH,'validset.csv'), word_dict, n_workers=CPUNUM)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset(os.path.join(DATA_PATH,'testset.csv'), word_dict, n_workers=CPUNUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dataset class for the abstract dataset\n",
    "`torch.utils.data.Dataset` is an abstract class representing a dataset.<br />Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "- `__len__` so that len(dataset) returns the size of the dataset.\n",
    "- `__getitem__` to support the indexing such that dataset[i] can be used to get i\n",
    "th sample\n",
    "- `collate_fn` Users may use customized collate_fn to achieve custom batching\n",
    "    - Here we pad sequences of various lengths (make same length of every single sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx, max_len = 64):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(data['Abstract']) for data in datas])\n",
    "        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "        batch_abstract = []\n",
    "        batch_label = []\n",
    "        sent_len = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            pad_abstract = []\n",
    "            for sentence in data['Abstract']:\n",
    "                if len(sentence) > max_len:\n",
    "                    pad_abstract.append(sentence[:max_len])\n",
    "                else:\n",
    "                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "            sent_len.append(len(pad_abstract))\n",
    "            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "            batch_abstract.append(pad_abstract)\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                pad_label = data['Label']\n",
    "                pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n",
    "                \n",
    "                batch_label.append(pad_label)\n",
    "        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 64)\n",
    "validData = AbstractDataset(valid, PAD_TOKEN, max_len = 64)\n",
    "testData = AbstractDataset(test, PAD_TOKEN, max_len = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO-DO List\n",
    "- [x] : Increase GRU layer  \n",
    "        Now : 2 layer\n",
    "- [ ] : use Fast text embedding\n",
    "- [ ] : use `title` column\n",
    "- [x] : add dropout layer\n",
    "        Now : dropout prob = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim, _weight=embedding_matrix)\n",
    "        self.sent_rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=2, dropout=drop_p, bidirectional=True, batch_first=True)\n",
    "        self.FCLayer = nn.Sequential(OrderedDict([\n",
    "            ('FC1', nn.Linear(hidden_dim*2, hidden_dim)),\n",
    "            ('DropOut1', nn.Dropout(drop_p)),\n",
    "            ('ReLU1', nn.ReLU()),\n",
    "            ('FC2', nn.Linear(hidden_dim, 6)),\n",
    "            ('Sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "        torch.nn.init.xavier_normal_(self.FCLayer[0].weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # b: batch_size\n",
    "        # s: number of sentences\n",
    "        # w: number of words\n",
    "        # e: embedding_dim\n",
    "        x = self.embedding(x)\n",
    "        b,s,w,e = x.shape\n",
    "        x = x.view(b,s*w,e)\n",
    "        x, __ = self.sent_rnn(x)\n",
    "        x = x.view(b,s,w,-1)\n",
    "        x = torch.max(x,dim=2)[0]\n",
    "        y = self.FCLayer(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions for scoring\n",
    "\n",
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.5\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.bool) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(epoch, mode):\n",
    "    model.train(True)\n",
    "    if mode==\"train\":\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=8)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    for i, (x, y, sent_len) in trange:\n",
    "        if epoch == 0:\n",
    "            writer.add_graph(model, x.to(device))\n",
    "\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if mode==\"train\":\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    \n",
    "    if mode==\"train\":\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        writer.add_scalar('Loss/train', loss/ len(trange), epoch)\n",
    "        writer.add_scalar('F1_score/train', f1_score.get_score(), epoch)\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        writer.add_scalar('Loss/valid', loss/ len(trange), epoch)\n",
    "        writer.add_scalar('F1_score/valid', f1_score.get_score(), epoch)\n",
    "    trange.close()\n",
    "    \n",
    "\n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists(os.path.join(CWD,'model')):\n",
    "        os.makedirs(os.path.join(CWD,'model'))\n",
    "    torch.save(model.state_dict(), os.path.join( CWD,'model/model.pkl.'+str(epoch) ))\n",
    "    with open( os.path.join( CWD,'model/history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 0\n\nTrain:   0%|          | 0/394 [00:00<?, ?it/s]\u001b[A\nTrain:   0%|          | 0/394 [00:11<?, ?it/s, f1=0.13492, loss=0.682]\u001b[A\nTrain:   0%|          | 1/394 [00:11<1:15:33, 11.54s/it, f1=0.13492, loss=0.682]\u001b[A\nTrain:   0%|          | 1/394 [00:18<1:15:33, 11.54s/it, f1=0.10843, loss=0.675]\u001b[A\nTrain:   1%|          | 2/394 [00:18<1:05:55, 10.09s/it, f1=0.10843, loss=0.675]\u001b[A\nTrain:   1%|          | 2/394 [00:26<1:05:55, 10.09s/it, f1=0.09091, loss=0.668]\u001b[A\nTrain:   1%|          | 3/394 [00:26<1:01:42,  9.47s/it, f1=0.09091, loss=0.668]\u001b[A\nTrain:   1%|          | 3/394 [00:36<1:01:42,  9.47s/it, f1=0.07852, loss=0.661]\u001b[A\nTrain:   1%|          | 4/394 [00:36<1:02:14,  9.58s/it, f1=0.07852, loss=0.661]\u001b[A"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2da34ddd142f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-31c74af58c8c>\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(epoch, mode)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mo_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;31m# A valid PyTorch model should have a 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;31m# Caffe2 models do not have the 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    856\u001b[0m         return trace_module(func, {'forward': example_inputs}, None,\n\u001b[1;32m    857\u001b[0m                             \u001b[0mcheck_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                             check_tolerance, _force_outplace, _module_class)\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                 _check_trace([inputs], func, check_trace_method,\n\u001b[0;32m-> 1007\u001b[0;31m                              check_tolerance, _force_outplace, True, _module_class)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36m_check_trace\u001b[0;34m(check_inputs, func, traced_func, check_tolerance, force_outplace, is_trace_module, _module_class)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mfn_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mod_and_filter_tensor_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Python function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraced_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Python function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mcheck_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mod_and_filter_tensor_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_mod_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeated trace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mcompare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraced_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeated trace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AICUP2019_Abstract_Labeling/venv_Abstract_Labeling/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mrun_mod_and_filter_tensor_outputs\u001b[0;34m(mod, inputs, running_what)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrun_mod_and_filter_tensor_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_what\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_retval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_clone_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net(len(word_dict))\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "history = {'train':[],'valid':[]}\n",
    "\n",
    "## Tensorboard\n",
    "## save path: test_experiment/\n",
    "tf_path = os.path.join(CWD, 'test_experiment')\n",
    "if not os.path.exists(tf_path):\n",
    "    os.mkdir(tf_path)\n",
    "writer = SummaryWriter(os.path.join(tf_path,config_fname))\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    _run_epoch(epoch, 'train')\n",
    "    _run_epoch(epoch, 'valid')\n",
    "    save(epoch)\n",
    "\n",
    "# Plot the training results \n",
    "with open(os.path.join(CWD,'model/history.json'), 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}